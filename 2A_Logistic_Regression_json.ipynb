{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "2A_Logistic_Regression.json",
      "provenance": [],
      "collapsed_sections": [
        "tRtrFTykEIcE",
        "jcj3-ylXEIcG",
        "-vJK9LLhEIcI",
        "H0-trU4vEIcI",
        "DD9pkhWoEIcK",
        "wc7t64AiEIcL",
        "FeBjOx5GEIcM",
        "c5Gl_8seEIcN",
        "1D9qCoc3EIcO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanishrai/MNIST_Logistic_Regression/blob/main/2A_Logistic_Regression_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZonANDTEIbz"
      },
      "source": [
        "# Introduction to Logistic Regression in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kz4tVKXEIb6"
      },
      "source": [
        "In this notebook, we're going to build a very simple neural network in PyTorch to do handwritten digit classification.\n",
        "First, we'll start with some exploration of the MNIST dataset, explaining how we load and format the data.\n",
        "We'll then jump into motivating and then implementing the logistic regression model, including the forward and backwards pass, loss functions, and optimizers.\n",
        "After training the model, we'll evaluate how we did and visualize what we've learned.\n",
        "Finally, we'll refactor our code in an object-oriented manner, using higher level APIs.\n",
        "\n",
        "Before we get started, some imports for the packages we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZBVcdRCEIb7"
      },
      "source": [
        "#update by SR\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6jNBnpxEIb8"
      },
      "source": [
        "### MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI0eWGM5EIb8"
      },
      "source": [
        "The MNIST dataset is very popular machine learning dataset, consisting of 70000 grayscale images of handwritten digits, of dimensions 28x28. \n",
        "We'll be using it as our example dataset for this section of the tutorial, with the goal being to predict which digit is in each image.\n",
        "\n",
        "![mnist](mnist.png)\n",
        "\n",
        "\n",
        "The first (and often most important) step in machine learning is preparing the data.\n",
        "This can include downloading, organizing, formatting, shuffling, pre-processing, augmenting, and batching examples so that they can be fed to a model.\n",
        "The `torchvision` package makes this easy by implementing many of these, allowing us to put these datasets into a usable form in only a few lines of code.\n",
        "First, let's download the train and test sets of MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KJm40boEIb9"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb-tnGcHEIb9"
      },
      "source": [
        "print(\"Number of MNIST training examples: {}\".format(len(mnist_train)))\n",
        "print(\"Number of MNIST test examples: {}\".format(len(mnist_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co8xKwjpEIb-"
      },
      "source": [
        "As we'd expect, 60000 of the MNIST examples are in the train set, and the rest are in the test set.\n",
        "We added the transform `ToTensor()` when formatting the dataset, to convert the input data from a Pillow `Image` type into a PyTorch `Tensor`. Tensors will eventually be the input type that we feed into our model. \n",
        "\n",
        "Let's look at an example image from the train set and its label.\n",
        "Notice that the `image` tensor defaults to something 3-dimensional.\n",
        "The \"1\" in the first dimension indicates that the image only has one channel (i.e. grayscale).\n",
        "We need to get rid of this to visualize the image with `imshow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SA5GO8OQEIb-"
      },
      "source": [
        "# Pick out the 4th (0-indexed) example from the training set\n",
        "image, label = mnist_train[3]\n",
        "\n",
        "# Plot the image\n",
        "print(\"Default image shape: {}\".format(image.shape))\n",
        "image = image.reshape([28,28])\n",
        "print(\"Reshaped image shape: {}\".format(image.shape))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "\n",
        "# Print the label\n",
        "print(\"The label for this image: {}\".format(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdtXAyRxEIb-"
      },
      "source": [
        "While we could work directly with the data as a `torchvision.dataset`, we'll find it useful to use a `DataLoader`, which will take care of shuffling and batching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTWep2yeEIb_"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeXNgyApEIb_"
      },
      "source": [
        "An example of a minibatch drawn from a `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hYBgT0INEIb_"
      },
      "source": [
        "data_train_iter = iter(train_loader)\n",
        "images, labels = data_train_iter.next()\n",
        "\n",
        "print(\"Shape of the minibatch of images: {}\".format(images.shape))\n",
        "print(\"Shape of the minibatch of labels: {}\".format(labels.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9An7-TEwEIb_"
      },
      "source": [
        "### Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utg8P0XKEIcA"
      },
      "source": [
        "Now that we have a good feel for how to load our data, let's start putting together our model. \n",
        "In this tutorial, we'll be building a logistic regression model, which is essentially a fully-connected neural network without any hidden layers. \n",
        "While fairly basic, logistic regression can perform surprisingly well on many simple classification tasks.\n",
        "\n",
        "#### The forward pass\n",
        "\n",
        "While our data inputs (which we'll call `x`) are images (i.e. 2-dimensional), MNIST digits are pretty small, and the model we're using is very simple.\n",
        "Thus, we're going to be treating the input as flat vectors.\n",
        "To convert our inputs into row vectors (a.k.a. flattening), we can use `view()`, the equivalent of NumPy's `reshape()`.\n",
        "Also like NumPy, we can replace one of the dimensions of the reshaping with a `-1`, which tells PyTorch to infer this dimension based on the original dimensions and the other specified dimensions.\n",
        "Let's do try this flattening on the minibatch of 100 images we drew in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXn1e0Y2EIcA"
      },
      "source": [
        "x = images.view(-1, 28*28)\n",
        "print(\"The shape of input x: {}\".format(x.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXP0KE3_EIcA"
      },
      "source": [
        "To get our predicted probabilities of each digit, let's first start with the probability of a digit being a 1 like the image above. \n",
        "For our simple model, we can start by applying a linear transformation. \n",
        "That is, we multiply each pixel $x_i$ of the input row vector by a weight $w_{i,1}$, sum them all together, and then add a bias $b_1$.\n",
        "This is equivalent to a dot product between the class \"1\" weights and the input:\n",
        "\n",
        "\\begin{align}\n",
        "y_1 = \\sum_i x_i w_{i,1} + b_1\n",
        "\\end{align}\n",
        "\n",
        "The magnitude of this result $y_1$, we'll take as being correlated to our belief in how likely we think the input digit was a 1. \n",
        "The higher the value of $y_1$, the more likely we think the input image $x$ was a 1 (i.e., we'd hope we'd get a relatively large value for $y_1$ for the above image). \n",
        "Remember though, our original goal was to identify all 10 digits, so we actually have:\n",
        "\n",
        "\\begin{align*}\n",
        "y_0 =& \\sum_i x_i w_{i,0} + b_0 \\\\\n",
        "y_1 =& \\sum_i x_i w_{i,1} + b_1 \\\\\n",
        "y_2 =& \\sum_i x_i w_{i,2} + b_2 \\\\\n",
        "y_3 =& \\sum_i x_i w_{i,3} + b_3 \\\\\n",
        "y_4 =& \\sum_i x_i w_{i,4} + b_4 \\\\\n",
        "y_5 =& \\sum_i x_i w_{i,5} + b_5 \\\\\n",
        "y_6 =& \\sum_i x_i w_{i,6} + b_6 \\\\\n",
        "y_7 =& \\sum_i x_i w_{i,7} + b_7 \\\\\n",
        "y_8 =& \\sum_i x_i w_{i,8} + b_8 \\\\\n",
        "y_9 =& \\sum_i x_i w_{i,9} + b_9\n",
        "\\end{align*}\n",
        "\n",
        "We can express this in matrix form as:\n",
        "\n",
        "\\begin{align}\n",
        "y = x W + b \n",
        "\\end{align}\n",
        "\n",
        "To take advantage of parallel computation, we commonly process multiple inputs $x$ at once, in a minibatch.\n",
        "We can stack each input $x$ into a matrix $X$, giving us \n",
        "\n",
        "\\begin{align}\n",
        "Y = X W + b \n",
        "\\end{align}\n",
        "\n",
        "Visualizing the dimensions:\n",
        "\n",
        "<img src=\"Figures/mnist_matmul.PNG\" width=\"500\"/>\n",
        "\n",
        "In our specific example, the minibatch size $m$ is $100$, the dimension of the data is $28 \\times 28=784$, and the number of classes $c$ is $10$.\n",
        "While $X$ and $Y$ are matrices due to the batching, conventionally, they are often given lowercase variable names, as if they were for a single example.\n",
        "We will use `x` and `y` throughout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWRgAzLaEIcB"
      },
      "source": [
        "The weight $W$ and bias $b$ make up the parameters of this model.\n",
        "When we say that we want to \"learn the model,\" what we're really trying to do is find good values for every element in $W$ and $b$.\n",
        "Before we begin learning, we need to initialize our parameters to some value, as a starting point.\n",
        "Here, we don't really know what the best values are, so we going to initialize $W$ randomly (using something called [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a.html)), and set $b$ to a vector of zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMRfFFamEIcB"
      },
      "source": [
        "# Randomly initialize weights W\n",
        "W = torch.randn(784, 10)/np.sqrt(784)\n",
        "W.requires_grad_()\n",
        "\n",
        "# Initialize bias b as 0s\n",
        "b = torch.zeros(10, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PGiAhlTEIcC"
      },
      "source": [
        "As both `W` and `b` are parameters we wish to learn, we set `requires_grad` to `True`. \n",
        "This tells PyTorch's autograd to track the gradients for these two variables, and all the variables depending on `W` and `b`.\n",
        "\n",
        "With these model parameters, we compute $y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUEE87KhEIcC"
      },
      "source": [
        "# Linear transformation with W and b\n",
        "y = torch.matmul(x, W) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzue7LTiEIcC"
      },
      "source": [
        "We can see for example what the predictions look like for the first example in our minibatch. Remember, the bigger the number, the more the model thinks the input $x$ is of that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vsLQLCw0EIcC"
      },
      "source": [
        "print(y[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZuTSA-LEIcD"
      },
      "source": [
        "We can interpret these values (aka logits) $y$ as probabilities if we normalize them to be positive and add up to 1. In logistic regression, we do this with a softmax:\n",
        "\n",
        "\\begin{align}\n",
        "p(y_i) = \\text{softmax}(y_i) = \\frac{\\text{exp}(y_i)}{\\sum_j\\text{exp}(y_j)}\n",
        "\\end{align}\n",
        "\n",
        "Notice that because the range of the exponential function is always non-negative, and since we're normalizing by the sum, the softmax achieves the desired property of producing values between 0 and 1 that sum to 1. If we look at the case with only 2 classes, we see that the softmax is the multi-class extension of the binary sigmoid function: \n",
        "\n",
        "<img src=\"Figures/Logistic-curve.png\" width=\"300\"/>\n",
        "\n",
        "We can compute the softmax ourselves using the above formula if we'd like, but PyTorch already has the softmax function in `torch.nn.functional`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isEnmsP1EIcD"
      },
      "source": [
        "# Option 1: Softmax to probabilities from equation\n",
        "py_eq = torch.exp(y) / torch.sum(torch.exp(y), dim=1, keepdim=True)\n",
        "print(\"py[0] from equation: {}\".format(py_eq[0]))\n",
        "\n",
        "# Option 2: Softmax to probabilities with torch.nn.functional\n",
        "import torch.nn.functional as F\n",
        "py = F.softmax(y, dim=1)\n",
        "print(\"py[0] with torch.nn.functional.softmax: {}\".format(py[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpthSADBEIcD"
      },
      "source": [
        "We've now defined the forward pass of our model: given an input image, the graph returns the probabilities the model thinks the input is each of the 10 classes. Are we done?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtrFTykEIcE"
      },
      "source": [
        "#### The cross-entropy loss\n",
        "\n",
        "This tutorial isn't done yet, so you can probably guess that the answer is not quite. \n",
        "We don't know the values of $W$ and $b$ yet! \n",
        "Remember how we initialized them randomly? \n",
        "Before we adjust any of the weights, we need a way to measure how the model is doing.\n",
        "Specifically, we're going to measure how badly the model is doing.\n",
        "We do this with a *loss* function, which takes the model's prediction and returns a single number (i.e. a scalar) summarizing model performance.\n",
        "This loss will inform how we adjust the parameters of the model.\n",
        "\n",
        "The loss we commonly use in classification is cross-entropy, a concept from information theory.\n",
        "Explaining exactly what the cross-entropy represents goes slightly beyond the scope of this course, but you can think of it as a way of quantifying how far apart one distribution $y'$ is from another $y$.\n",
        "\n",
        "\\begin{align}\n",
        "H_{y'}(y)=-\\sum_i y'_i \\text{log}(y_i)\n",
        "\\end{align}\n",
        "\n",
        "In our case, $y$ is the set of probabilities predicted by the model ($py$ above); $y'$ is the target distribution.\n",
        "What is the target distribution?\n",
        "It's the true label, which is what we wanted the model to predict.\n",
        "\n",
        "Cross-entropy not only captures how *correct* (max probability corresponds to the right answer) the model's answers are, it also accounts for how *confident* (high confidence in correct answers) they are. This encourages the model to produce very high probabilities for correct answers while driving down the probabilities for the wrong answers, instead of merely being satisfied with it being the argmax. \n",
        "\n",
        "We focus here on supervised learning, a setting in which we have the labels.\n",
        "Our `DataLoader` automatically includes the corresponding labels for each of our inputs.\n",
        "Here are the labels from the first time we retrieved a minibatch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKeM6SnNEIcF"
      },
      "source": [
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak6NdxaBEIcF"
      },
      "source": [
        "Like the softmax operation, we can implement the cross-entropy directly from the equation, using the softmax output.\n",
        "However, as with the softmax, `torch.nn.functional` already has the cross-entropy loss implemented as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iTF6hl7EIcF"
      },
      "source": [
        "# Cross-entropy loss from equation\n",
        "cross_entropy_eq = torch.mean(-torch.log(py_eq)[range(labels.shape[0]),labels])\n",
        "print(\"cross entropy from equation: {}\".format(cross_entropy_eq))\n",
        "\n",
        "# Option 2: cross-entropy loss with torch.nn.functional\n",
        "cross_entropy = F.cross_entropy(y, labels)\n",
        "print(\"cross entropy with torch.nn.functional.cross_entropy: {}\".format(cross_entropy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXbBsw-EIcF"
      },
      "source": [
        "Note that PyTorch's cross-entropy loss combines the softmax operator and cross-entropy into a single operation, for numerical stability reasons.\n",
        "Don't do the softmax twice!\n",
        "Make sure to feed in the pre-softmax logits `y`, not the post-softmax probabilities `py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcj3-ylXEIcG"
      },
      "source": [
        "#### The backwards pass\n",
        "\n",
        "Now that we have the loss as a way of quantifying how badly the model is doing, we can improve our model by changing the parameters in a way that minimizes the loss.\n",
        "For neural networks, the common way of doing this is with backpropagation: we take the gradient of the loss with respect to $W$ and $b$ and take a step in the direction that reduces our loss.\n",
        "\n",
        "If we were not using a deep learning framework like PyTorch, we would have to go through and derive all the gradients ourselves by hand, then code them into our program. \n",
        "We certainly still could.\n",
        "However, with modern auto-differentiation libraries, it's much faster and easier to let the computer do it.\n",
        "\n",
        "First, we need to create an optimizer.\n",
        "There are many choices, but since logistic regression is fairly simple, we'll use standard stochastic gradient descent (SGD), which makes the following update:\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta \\mathcal{L}\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta$ is a parameter, $\\alpha$ is our learning rate (step size), and $\\nabla_\\theta \\mathcal{L}$ is the gradient of our loss with respect to $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJzCAMnQEIcG"
      },
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.SGD([W,b], lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4zij0ThEIcG"
      },
      "source": [
        "When we created our parameters $W$ and $b$, we indicated that they require gradients.\n",
        "To compute the gradients for $W$ and $b$, we call the `backward()` function on the cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8r7vX8XEIcG"
      },
      "source": [
        "cross_entropy.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4PCayV8EIcG"
      },
      "source": [
        "Each of the variables that required gradients have now accumulated gradients.\n",
        "We can see these for example on `b`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rVSaBtQzEIcH"
      },
      "source": [
        "b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMUc6iieEIcH"
      },
      "source": [
        "To apply the gradients, we could manually update $W$ and $b$ using the update rule $\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta \\mathcal{L}$, but since we have an optimizer, we can tell it to perform the update step for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgod1Y2IEIcH"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxYzW0pIEIcH"
      },
      "source": [
        "We set our learning rate to 0.1, so `b` has been updated by `-0.1*b.grad`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ROfSr68mEIcH"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsDFSNJuEIcI"
      },
      "source": [
        "We've now successfully trained on a minibatch!\n",
        "However, one minibatch probably isn't enough.\n",
        "At this point, we've trained the model on 100 examples out of the 60000 in the training set.\n",
        "We're going to need to repeat this process, for more of the data.\n",
        "\n",
        "One more thing to keep in mind though: gradients calculated by `backward()` don't override the old values; instead, they accumulate.\n",
        "Therefore, you'll want to clear the gradient buffers before you compute gradients for the next minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4qsJGyZEIcI"
      },
      "source": [
        "print(\"b.grad before zero_grad(): {}\".format(b.grad))\n",
        "optimizer.zero_grad()\n",
        "print(\"b.grad after zero_grad(): {}\".format(b.grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vJK9LLhEIcI"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rbEBh8KEIcI"
      },
      "source": [
        "To train the model, we just need repeat what we just did for more minibatches from the training set.\n",
        "As a recap, the steps were:\n",
        "1. Draw a minibatch\n",
        "2. Zero the gradients in the buffers for `W` and `b`\n",
        "3. Perform the forward pass (compute prediction, calculate loss)\n",
        "4. Perform the backward pass (compute gradients, perform SGD step)\n",
        "\n",
        "Going through the entire dataset once is referred to as an epoch.\n",
        "In many cases, we train neural networks for multiple epochs, but here, a single epoch is enough.\n",
        "We also wrap the train_loader with `tqdm`.\n",
        "This isn't neccessary, but it adds a handy progress bar so we can track our training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZHkppxuEIcI"
      },
      "source": [
        "# Iterate through train set minibatchs \n",
        "for images, labels in tqdm(train_loader):\n",
        "    # Zero out the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    x = images.view(-1, 28*28)\n",
        "    y = torch.matmul(x, W) + b\n",
        "    cross_entropy = F.cross_entropy(y, labels)\n",
        "    # Backward pass\n",
        "    cross_entropy.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0-trU4vEIcI"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aO9k8K6EIcJ"
      },
      "source": [
        "Now let's see how we did! \n",
        "For every image in our test set, we run the data through the model, and take the digit in which we have the highest confidence as our answer. \n",
        "We then compute an accuracy by seeing how many we got correct.\n",
        "We're going to wrap evaluation with `torch.no_grad()`, as we're not interested in computing gradients during evaluation.\n",
        "By turning off the autograd engine, we can speed up evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJs3Bl1oEIcJ"
      },
      "source": [
        "correct = 0\n",
        "total = len(mnist_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Iterate through test set minibatchs \n",
        "    for images, labels in tqdm(test_loader):\n",
        "        # Forward pass\n",
        "        x = images.view(-1, 28*28)\n",
        "        y = torch.matmul(x, W) + b\n",
        "        \n",
        "        predictions = torch.argmax(y, dim=1)\n",
        "        correct += torch.sum((predictions == labels).float())\n",
        "    \n",
        "print('Test accuracy: {}'.format(correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j7uEVknEIcJ"
      },
      "source": [
        "Not bad for a simple model and a few lines of code.\n",
        "Before we conclude this example, there's one more interesting thing we can do. \n",
        "Normally, it can be difficult to inspect exactly what the filters in a model are doing, but since this model is so simple, and the weights transform the data directly to their logits, we can actually visualize what the model's learning by simply plotting the weights. \n",
        "The results look pretty reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qmDwHc6KEIcJ"
      },
      "source": [
        "# Get weights\n",
        "fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
        "\n",
        "for digit in range(10):\n",
        "    ax[digit].imshow(W[:,digit].detach().view(28,28), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSP2TEZPEIcJ"
      },
      "source": [
        "As we can see, the model learned a template for each digit.\n",
        "Remember that our model takes a dot product between the weights of each digit and input.\n",
        "Therefore, the more the input matches the template for a digit, the higher the value of the dot product for that digit will be, which makes the model more likely to predict that digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD9pkhWoEIcK"
      },
      "source": [
        "#### The Full Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw9tqfU2EIcK"
      },
      "source": [
        "The entire model, with the complete model definition, training, and evaluation (but minus the weights visualization) as independently runable code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP4RwI_jEIcK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load the data\n",
        "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
        "\n",
        "## Training\n",
        "# Initialize parameters\n",
        "W = torch.randn(784, 10)/np.sqrt(784)\n",
        "W.requires_grad_()\n",
        "b = torch.zeros(10, requires_grad=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD([W,b], lr=0.1)\n",
        "\n",
        "# Iterate through train set minibatchs \n",
        "for images, labels in tqdm(train_loader):\n",
        "    # Zero out the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    x = images.view(-1, 28*28)\n",
        "    y = torch.matmul(x, W) + b\n",
        "    cross_entropy = F.cross_entropy(y, labels)\n",
        "    # Backward pass\n",
        "    cross_entropy.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "## Testing\n",
        "correct = 0\n",
        "total = len(mnist_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Iterate through test set minibatchs \n",
        "    for images, labels in tqdm(test_loader):\n",
        "        # Forward pass\n",
        "        x = images.view(-1, 28*28)\n",
        "        y = torch.matmul(x, W) + b\n",
        "        \n",
        "        predictions = torch.argmax(y, dim=1)\n",
        "        correct += torch.sum((predictions == labels).float())\n",
        "    \n",
        "print('Test accuracy: {}'.format(correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsYyqPvEIcK"
      },
      "source": [
        "Note: The accuracy from the full version directly above might return a slightly different test accuracy from the step-by-step version we first went through. \n",
        "We trained our model with stochastic gradient descent (SGD), with the word \"stochastic\" highlighting that training is an inherently random process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgSnaiYBEIcL"
      },
      "source": [
        "### Higher level APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc7t64AiEIcL"
      },
      "source": [
        "So far, we've primarily been building neural networks with fairy basic PyTorch operations.\n",
        "We did this to provide a clearer picture of how models actually work and what's going on under the hood.\n",
        "This can be important as you're learning concepts and the various frameworks, and sometimes the low-level control is necessary if you're trying to build something novel.\n",
        "\n",
        "However, most of the time, we do find ourselves repeating the same fairly standard lines of code, which can slow us down. \n",
        "Worse, it clutters up our code unnecessarily and introduces room for bugs and typos.\n",
        "And finally, as researchers or engineers, we would like to spend most of our time thinking on the highest levels of abstractions: I want to add a convolution layer here, then a fully-connected there, etc.\n",
        "Having to code all the small details are distractions that can detract from our ability to translate ideas into code.\n",
        "For this reason, PyTorch has higher level abstractions to help speed up implementation and improve model organization.\n",
        "While there are many ways to organize PyTorch code, one common paradigm is with `torch.nn.Module`.\n",
        "\n",
        "#### Object-oriented Refactorization\n",
        "\n",
        "It often makes sense for us to code our models in an [object-oriented manner](https://realpython.com/python3-object-oriented-programming/).\n",
        "To understand why, let's look back at the linear transformation $y = xW + b$ that we used for logistic regression. \n",
        "We can see that while the operation consisted of a matrix multiplication and addition, also associated with this operation was the instantiation of two parameters `W` and `b`, and these two parameters conceptually *belong* to the transform.\n",
        "As such, it would make sense to bundle up the instantiation of the two parameters with the actual transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0RwiLAEEIcL"
      },
      "source": [
        "# Note: illustrative example only; see below for torch.nn usage\n",
        "class xW_plus_b:\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        self.W = torch.randn(dim_in, dim_out)/np.sqrt(dim_in)\n",
        "        self.W.requires_grad_()\n",
        "        self.b = torch.zeros(dim_out, requires_grad=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.W) + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_frvAxdSEIcL"
      },
      "source": [
        "To use what we just wrote, we can create an `xW_plus_b` instance using its `__init__()` method (the constructor).\n",
        "In this case, we're going to set the dimensions to be 784 and 10, as we did in our logisitic regression example above.\n",
        "This creates an `xW_plus_b` instance with two parameters `W` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfDDX4S-EIcM"
      },
      "source": [
        "# Note: illustrative example only; see below for torch.nn usage\n",
        "lin_custom = xW_plus_b(784, 10)\n",
        "print(\"W: {}\".format(lin_custom.W.shape))\n",
        "print(\"b: {}\".format(lin_custom.b.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIimPgkEIcM"
      },
      "source": [
        "After instantiating the instance, we can perform the actual linear transform of our custom `xW_plus_b` class by calling the instance's `forward()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g24Qk4s4EIcM"
      },
      "source": [
        "# Note: illustrative example only; see below for torch.nn usage\n",
        "x_rand = torch.randn(1,784)\n",
        "y = lin_custom.forward(x_rand)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeBjOx5GEIcM"
      },
      "source": [
        "#### Using  `torch.nn`\n",
        "\n",
        "While we can certainly implement our own classes for the operations we'd like to use, we don't have to, as PyTorch already has them in the `torch.nn` sublibrary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyct39ueEIcM"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHHZtalHEIcN"
      },
      "source": [
        "For example, the linear transform example we just went through is called `torch.nn.Linear`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GNqzOBFEIcN"
      },
      "source": [
        "lin = nn.Linear(784, 10)\n",
        "print(\"Linear parameters: {}\".format([p.shape for p in lin.parameters()]))\n",
        "\n",
        "y = lin(x_rand)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMhBO_jOEIcN"
      },
      "source": [
        "The implementation for `nn.Linear` has a few more things under the hood (notice for example that the `forward()` function is aliased with calling the instance itself), but in spirit, it operates in much the same way as our custom `xW_plus_b` class.\n",
        "In the first line, we instantiate a `Linear` object, which automatically creates weight and bias variables of the specified dimensions.\n",
        "The fourth line then calls the `forward()` function (aliased with the object call), which performs the linear transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Gl_8seEIcN"
      },
      "source": [
        "#### Using `torch.nn.Module`\n",
        "\n",
        "The `torch.nn.Linear` class we just saw is a subclass of `torch.nn.Module`.\n",
        "However, `Module`s do not have to just describe a single operation; they can also define a chain of operations, each of which may also be `Module`s.\n",
        "As such, we can place our entire neural network within a `Module`.\n",
        "In this case, the module can track all of its associated parameters, some of which may also be associated with a submodule (e.g. `nn.Linear`), while also defining the `forward()` function, in one place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvXXDtozEIcN"
      },
      "source": [
        "class MNIST_Logistic_Regression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWj_1JOWEIcN"
      },
      "source": [
        "In this particular example, we didn't need to chain any operations, but we'll see this come in handy as we move on to more complex models.\n",
        "Additionally, the `nn.Module` that we subclassed has a few other nice features.\n",
        "For example:\n",
        "- The `forward()` function of a `nn.Module` will call the `forward()` function of any child `nn.Module`s.\n",
        "- `print()` will print out a formatted summary of our model, recursively summarizing any child `nn.Module`s as well.\n",
        "- The `parameters()` function will return a generator that returns all parameters of a `nn.Module` (including those of any children)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8_EyfEtEIcO"
      },
      "source": [
        "model = MNIST_Logistic_Regression()\n",
        "y = model(x_rand)\n",
        "print(\"The model: \\n{}\".format(model))\n",
        "print(\"\\nParameters: \\n{}\".format(list(model.parameters())))\n",
        "print(\"\\nOutput shape: \\n{}\".format(y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D9qCoc3EIcO"
      },
      "source": [
        "#### Full code with `nn.Module`\n",
        "\n",
        "Refactoring our previous complete logistic regression code to use a `nn.Module`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RaoToNEIcO"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class MNIST_Logistic_Regression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "# Load the data\n",
        "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
        "\n",
        "## Training\n",
        "# Instantiate model\n",
        "model = MNIST_Logistic_Regression()\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Iterate through train set minibatchs \n",
        "for images, labels in tqdm(train_loader):\n",
        "    # Zero out the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    x = images.view(-1, 28*28)\n",
        "    y = model(x)\n",
        "    loss = criterion(y, labels)\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "## Testing\n",
        "correct = 0\n",
        "total = len(mnist_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Iterate through test set minibatchs \n",
        "    for images, labels in tqdm(test_loader):\n",
        "        # Forward pass\n",
        "        x = images.view(-1, 28*28)\n",
        "        y = model(x)\n",
        "        \n",
        "        predictions = torch.argmax(y, dim=1)\n",
        "        correct += torch.sum((predictions == labels).float())\n",
        "    \n",
        "print('Test accuracy: {}'.format(correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqch0G22EIcO"
      },
      "source": [
        "While the benefits of organizing a model as a `nn.Module` may not be as obvious for a simple logistic regression model, such a programming style allows for much quicker and cleaner implementations for more complex models, as we'll see in later notebooks."
      ]
    }
  ]
}